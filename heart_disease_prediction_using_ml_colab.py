# -*- coding: utf-8 -*-
"""Heart Disease Prediction Using ML COLAB.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10rLbPZJEnQMqqePCwvzkakqHXmwGeYdz
"""



"""# Heart Disease Prediction

In this machine learning project, I have collected the dataset from Kaggle (https://www.kaggle.com/) and I will be using Machine Learning to predict whether any person is suffering from heart disease
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np,math
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib import rcParams
from matplotlib.cm import rainbow
# %matplotlib inline
import warnings
warnings.filterwarnings('ignore')

"""Here we will be experimenting with 4 algorithms

    1.KNeighbors Classifier
    2.Decision Tree Classifier
    3.Gaussian Naive Bayes Classifier
    4.Support Vector Machine Classifier
    5.Random Forest Classifier

"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn import svm
from sklearn.ensemble import RandomForestClassifier

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
from sklearn import metrics as mt

!pip install -U -q PyDrive

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials


# Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

link = 'https://drive.google.com/file/d/1oowUt3c-HB5QItzbNjbeZGgTxjcD0snQ/view'

# to get the id part of the file
id = link.split("/")[-2]

downloaded = drive.CreateFile({'id':id})
downloaded.GetContentFile('heart.csv')

df = pd.read_csv('heart.csv')
df = df.sample(frac=1)

df.head()

df.shape

df.columns

df.info()

"""# Details Of DataSet

1.  age - age in years
2.  sex - (1 = male; 0 = female)
3.  cp - chest pain type
        0: Typical angina: chest pain related decrease blood supply to the heart
        1: Atypical angina: chest pain not related to heart
        2: Non-anginal pain: typically esophageal spasms (non heart related)
        3: Asymptomatic: chest pain not showing signs of disease
4.  trestbps - resting blood pressure (in mm Hg on admission to the hospital) anything above 130-140 is typically cause for concern
5.  chol - serum cholestoral in mg/dl
        serum = LDL + HDL + .2 * triglycerides
        above 200 is cause for concern
6.  fbs - (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)
        '>126' mg/dL signals diabetes
7.  restecg - resting electrocardiographic results
        0: Nothing to note
        1: ST-T Wave abnormality
            can range from mild symptoms to severe problems
            signals non-normal heart beat
        2: Possible or definite left ventricular hypertrophy
            Enlarged heart's main pumping chamber
8.  thalach - maximum heart rate achieved
9.  exang - exercise induced angina (1 = yes; 0 = no)
10. oldpeak - ST depression induced by exercise relative to rest looks at stress of heart during excercise unhealthy heart will stress more
11. slope - the slope of the peak exercise ST segment
        0: Upsloping: better heart rate with excercise (uncommon)
        1: Flatsloping: minimal change (typical healthy heart)
        2: Downslopins: signs of unhealthy heart
12. ca - number of major vessels (0-3) colored by flourosopy
        colored vessel means the doctor can see the blood passing through
        the more blood movement the better (no clots)
13. thal - thalium stress result
        1,3: normal
        6: fixed defect: used to be defect but ok now
        7: reversable defect: no proper blood movement when excercising
14. target - have disease or not (1=yes, 0=no) (= the predicted attribute)
"""

df.describe()

df.corr()["target"].abs().sort_values(ascending=False)

correlation_matrix=df.corr()
top_corr_features = correlation_matrix.index
plt.figure(figsize=(15,10))

g=sns.heatmap(df[top_corr_features].corr(),annot=True,cmap="RdYlGn")

correlation_matrix=df.corr()
columns = np.full((correlation_matrix.shape[0],), True, dtype=bool)
for i in range(correlation_matrix.shape[0]):
    for j in range(i+1, correlation_matrix.shape[0]):
        if correlation_matrix.iloc[i,j] >= 0.9:
            if columns[j]:
                columns[j] = False
columns[-1]=True
selected_columns = df.columns[columns]
df = df[selected_columns]
df.head()

df.hist(figsize=(14,18))

"""It's always a good practice to work with a dataset where the target classes are of approximately equal size. Thus, let's check for the same."""

sns.set_style('whitegrid')
sns.countplot(x='target',data=df,palette='Dark2')

sns.countplot(x='sex', data=df, palette='husl', hue='target')

pd.crosstab(df['sex'], df['target'])

"""# Data Processing

After exploring the dataset, I observed that I need to convert some categorical variables into dummy variables and scale all the values before training the Machine Learning models. First, I'll use the get_dummies method to create dummy columns for categorical variables.
"""

final_dataset=pd.get_dummies(df,columns=['sex','cp','fbs','restecg','exang','slope','ca','thal'])

standardScaler=StandardScaler()
columns_to_scale=['age','trestbps','chol','thalach','oldpeak']
final_dataset[columns_to_scale]=standardScaler.fit_transform(final_dataset[columns_to_scale])

final_dataset.head()

"""# Test-Train Spliting"""

# Y=final_dataset["target"]
# X=final_dataset.drop(["target"],axis=1)

predictors=final_dataset.drop("target",axis=1)
target=final_dataset["target"]
X_train,X_test,Y_train,Y_test = train_test_split(predictors,target,test_size=0.25,random_state=14)

# Y_train=final_dataset['target'][:1000]
# X_train=final_dataset.drop(['target'],axis=1)
# X_test=X_train[1000:]
# X_train=X_train[:1000]
# Y_test=final_dataset['target'][1000:]

X_train.shape

Y_train.shape

X_test.shape

Y_test.shape

"""# KNeighbors Classifier"""

def print_prediction(K,tp):
    clf=KNeighborsClassifier(K,'uniform')
    clf.fit(X_train,Y_train)
    prediction=clf.predict(X_test)
    if tp==False:
        return mt.accuracy_score(Y_test,prediction,normalize=True)
    else:
        print("Predictions\n")
        print(prediction)

        print("\nAccuracy with normalize = ",mt.accuracy_score(Y_test,prediction,normalize=True))
        print(mt.classification_report(Y_test,prediction))
        print(mt.confusion_matrix(Y_test,prediction))

plt.figure(figsize=(15,10))
knn_value=[]
knn_score=[]
for i in range(3,int(math.sqrt(len(X_train))),2):
    a=print_prediction(i,False)
    knn_score.append(a)
    knn_value.append(i)

plt.plot([k for k in range(3,int(math.sqrt(len(X_train))),2)],knn_score,color='green')
j=0
for i in range(3,int(math.sqrt(len(X_train))),2):
    plt.text(i,knn_score[j],(i,knn_score[j]))
    j+=1

print("K value :- ",end=" ")
print(knn_value[knn_score.index(max(knn_score))])
print("Accuracy :- ",end=" ")
print(max(knn_score))

plt.xticks([i for i in range(3,int(math.sqrt(len(X_train))),2)])
plt.xlabel('Number of Neighbors (K)')
plt.ylabel('Scores')
plt.title('K Neighbors Classifier scores for different K values')

knn_classifier=KNeighborsClassifier()
parameters={'n_neighbors' : (3,23,1),
            'weights': ['uniform', 'distance'],
            'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}

grid_kn=GridSearchCV(estimator = knn_classifier,
                     param_grid = parameters,
                     scoring = 'accuracy',
                     cv = 10,
                     verbose = 1,
                     n_jobs = -1).fit(X_train,Y_train)

print(grid_kn.best_estimator_)
print(grid_kn.best_params_)
print(grid_kn.score(X_test,Y_test))

print(grid_kn.score(X_train,Y_train))

compare=[]
compare.append(['KNeighbors',grid_kn.score(X_train,Y_train),grid_kn.score(X_test,Y_test)])

"""# Decision Tree Classifier"""

params = {
        'max_leaf_nodes': list(range(2, 50)),
        'min_samples_split': [2, 3, 4]}

grid_dt = GridSearchCV(DecisionTreeClassifier(random_state=42),
                    params,
                    verbose=1,
                    cv=10).fit(X_train,Y_train)
print(grid_dt.best_params_)
print(grid_dt.best_estimator_)
# print(grid.predict(X_test))
print(mt.accuracy_score(Y_test,grid_dt.predict(X_test),normalize=True))
# print(grid.predict(X_test))
print(mt.accuracy_score(Y_train,grid_dt.predict(X_train),normalize=True))

compare.append(['DecisionTree', mt.accuracy_score(Y_train,grid_dt.predict(X_train),normalize=True),mt.accuracy_score(Y_test,grid_dt.predict(X_test),normalize=True)] )

"""# Gaussian Naive Bayes Classifier"""

def print_predict(classify,X_test):
    prediction=classify.predict(X_test)
#     print("Predictions\n")
#     print(prediction)
    print("\nAccuracy with normalize = ",mt.accuracy_score(Y_test,prediction,normalize=True))
    print(mt.classification_report(Y_test,prediction))
    print(mt.confusion_matrix(Y_test,prediction))
    return mt.accuracy_score(Y_test,prediction,normalize=True)

def gaussian(X_train,Y_train,X_test):
    classify=GaussianNB() # for gaussian
    classify.fit(X_train,Y_train)

    return print_predict(classify,X_test)

accuracy=gaussian(X_train,Y_train,X_test)

clf=GaussianNB().fit(X_train,Y_train)
compare.append(['Gaussian_NaiveBayes',mt.accuracy_score(Y_train,clf.predict(X_train),normalize=True),accuracy])

"""# Support Vector Machine Classifier"""

param_grid = {'C': [0.1, 1, 10, 100],
              'gamma': [1, 0.1, 0.01, 0.001],
              'kernel': ['linear']}

grid_sv = GridSearchCV(svm.SVC(), param_grid, refit = True, verbose = 3).fit(X_train,Y_train)

print(grid_sv.best_params_)
print(grid_sv.best_estimator_)
print(mt.accuracy_score(Y_test,grid_sv.predict(X_test),normalize=True))
print(mt.accuracy_score(Y_train,grid_sv.predict(X_train),normalize=True))

compare.append(['Support_Vector_Machine',mt.accuracy_score(Y_train,grid_sv.predict(X_train),normalize=True),mt.accuracy_score(Y_test,grid_sv.predict(X_test),normalize=True)])

"""# Random Forest"""

rfc=RandomForestClassifier(random_state=42)
param_grid = {
    'n_estimators': [250,450],
    'max_features': ['auto', 'sqrt','log2'],
    'max_depth' : [4,5,6,7,8,9,10,11,12],
    'criterion' :['gini','entropy']
}
CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 10).fit(X_train,Y_train)

print(CV_rfc.best_params_)
print(CV_rfc.best_estimator_)
print(mt.accuracy_score(Y_test,CV_rfc.predict(X_test),normalize=True))
print(mt.accuracy_score(Y_train,CV_rfc.predict(X_train),normalize=True))

compare.append(['Random_Forest',mt.accuracy_score(Y_train,CV_rfc.predict(X_train),normalize=True),mt.accuracy_score(Y_test,CV_rfc.predict(X_test),normalize=True)])

"""# Comparision"""

a=[]
b=[]
c=[]
for x in range(len(compare)):
    a.append(compare[x][0])
    b.append(compare[x][1])
    c.append(compare[x][2])
df=pd.DataFrame(columns=["algorithms","train","test"], data=[[a[x],b[x]*100,c[x]*100] for x in range(len(compare))])
df

sns.set(rc={'figure.figsize':(15,10)})

df = pd.melt(df, id_vars="algorithms", var_name="type", value_name="rate")
splot = sns.barplot(x="algorithms", y="rate", hue="type", data=df)

plt.xlabel("Algorithms")
plt.ylabel("Accuracy score")

for p in splot.patches:
    splot.annotate(format(p.get_height(),'.5f'),(p.get_x()+p.get_width()/2.,p.get_height()),ha='center',va='center',xytext=(0,-14),textcoords='offset points',bbox=dict(boxstyle="round4", alpha=0.5),)

"""# It is observed that  Random Forest is more acurate than the rest algorithms for this perticular dataset."""

